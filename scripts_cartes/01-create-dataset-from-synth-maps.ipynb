{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set main parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = \"/home/STual/DAN-cadastre\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create subsets and format named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List, Tuple\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def split_dataset(data: List[str], train_ratio: float, val_ratio: float, test_ratio: float, seed: int = None) -> Tuple[List[str], List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Splits the dataset into train, validation, and test sets based on the given ratios.\n",
    "\n",
    "    Args:\n",
    "        data (List[str]): List of data samples (e.g., file paths).\n",
    "        train_ratio (float): Proportion of data to use for the training set.\n",
    "        val_ratio (float): Proportion of data to use for the validation set.\n",
    "        test_ratio (float): Proportion of data to use for the test set.\n",
    "        seed (int, optional): Random seed for reproducibility. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[str], List[str], List[str]]: A tuple containing the train, validation, and test sets.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "\n",
    "    # Ensure the ratios sum to 1.0\n",
    "    assert train_ratio + val_ratio + test_ratio == 1.0, \"Train, validation, and test ratios must sum to 1.0\"\n",
    "\n",
    "    # Shuffle the data\n",
    "    random.shuffle(data)\n",
    "\n",
    "    # Calculate split indices\n",
    "    total = len(data)\n",
    "    train_end = int(train_ratio * total)\n",
    "    val_end = train_end + int(val_ratio * total)\n",
    "\n",
    "    # Split the data\n",
    "    train_data = data[:train_end]\n",
    "    val_data = data[train_end:val_end]\n",
    "    test_data = data[val_end:]\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "\n",
    "# Example usage\n",
    "data_samples = glob.glob(os.path.join(path, '*.png'))\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.2\n",
    "seed = 42\n",
    "\n",
    "train_data, val_data, test_data = split_dataset(data_samples, train_ratio, val_ratio, test_ratio, seed)\n",
    "\n",
    "print(\"Train Data:\", train_data)\n",
    "print(\"Validation Data:\", val_data)\n",
    "print(\"Test Data:\", test_data)\n",
    "subsets = {'train': train_data, 'val': val_data, 'test': test_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using annotations.csv, can you concatenate the \"texte\" that have the same id zone. For each \"texte\", depending on the \"nature\" value, add the following tokens before and after the words\n",
    "#tokens => parcelle: start: Ⓐ end: Ⓑ\n",
    "        #=>nature: start: Ⓒ end: Ⓓ\n",
    "\n",
    "# Create a dictionary to store the concatenated text for each id_zone\n",
    "concatenated_text = {}\n",
    "\n",
    "# Iterate over the DataFrame\n",
    "for _, row in df.iterrows():\n",
    "    if row['id_zone'] in concatenated_text:\n",
    "        concatenated_text[row['id_zone']] += ' '\n",
    "        if row['nature'] == 'parcelle':\n",
    "            concatenated_text[row['id_zone']] += 'Ⓐ' + row['texte'] + 'Ⓑ'\n",
    "        elif row['nature'] == 'rue' or row['nature'] == 'commune' or row['nature'] == 'riviere':\n",
    "            concatenated_text[row['id_zone']] += 'Ⓒ' + row['texte'] + 'Ⓓ'\n",
    "    else:\n",
    "        if row['nature'] == 'parcelle':\n",
    "            concatenated_text[row['id_zone']] = 'Ⓐ' + row['texte'] + 'Ⓑ'\n",
    "        elif row['nature'] == 'rue' or row['nature'] == 'commune' or row['nature'] == 'riviere':\n",
    "            concatenated_text[row['id_zone']] = 'Ⓒ' + row['texte'] + 'Ⓓ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create split.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "splitjson = { \"test\" : {},\n",
    "\"val\" : {},\n",
    "\"train\" : {}\n",
    "}\n",
    "\n",
    "for subset in subsets:\n",
    "    set_ = subset\n",
    "    dataset = subsets[subset]\n",
    "\n",
    "    for elem in dataset:\n",
    "        fpath = elem.split(\"/\")[-1]\n",
    "        #use this regex _(.*).png with re lib\n",
    "        pattern = re.compile(r'_(.*)\\.png')\n",
    "        match = pattern.search(fpath)\n",
    "        id = match.group(1)\n",
    "        subjson = {id:{\"dataset_id\":\"synth_maps_dataset\",\"image\":{},\"text\":{}}}\n",
    "        subjson[id][\"image\"][\"iiif_url\"] = elem\n",
    "        subjson[id][\"image\"][\"polygon\"] = [\n",
    "                    [0,0],\n",
    "                    [0,2000],\n",
    "                    [2000,2000],\n",
    "                    [2000,0],[0,0]\n",
    "                ]\n",
    "        subjson[id][\"text\"] = concatenated_text[id]\n",
    "        splitjson[set_].update(subjson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write finaljson as split.json file\n",
    "import json\n",
    "with open(f\"{ROOT}/dataset-cartes/synth_maps_dataset/split.json\", 'w') as f:\n",
    "    json.dump(splitjson, f, indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create labels.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the train_data, val_data and test_data, can you create a new folder for each subset, that contains a synth_maps_dataset folder, and copy the images in this last corresponding folder\n",
    "import shutil\n",
    "for subset in subsets:\n",
    "    \n",
    "    dataset = subsets[subset]\n",
    "    for elem in dataset:\n",
    "        fpath = elem.split(\"/\")[-1]\n",
    "        #use this regex _(.*).png with re lib\n",
    "        pattern = re.compile(r'_(.*)\\.png')\n",
    "        match = pattern.search(fpath)\n",
    "        id = match.group(1)\n",
    "        os.makedirs(f\"{ROOT}/dataset-cartes/synth_maps_dataset/images/{subset}\", exist_ok=True)\n",
    "        os.makedirs(f\"{ROOT}/dataset-cartes/synth_maps_dataset/images/{subset}/synth_maps_dataset\", exist_ok=True)\n",
    "        shutil.copy(elem, f\"{ROOT}/dataset-cartes/synth_maps_dataset/images/{subset}/synth_maps_dataset/\"+fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsjson = { \"test\" : {},\n",
    "\"val\" : {},\n",
    "\"train\" : {}\n",
    "}\n",
    "\n",
    "for subset in subsets:\n",
    "    set_ = subset\n",
    "    dataset = subsets[subset]\n",
    "    for elem in dataset:\n",
    "        subjson = {}\n",
    "        fpath = elem.split(\"/\")[-1]\n",
    "        #use this regex _(.*).png with re lib\n",
    "        pattern = re.compile(r'_(.*)\\.png')\n",
    "        match = pattern.search(fpath)\n",
    "        id = match.group(1)\n",
    "        url = f\"images/{set_}/synth_maps_dataset/extrait_{id}.png\"\n",
    "        subjson[url] = concatenated_text[id]\n",
    "        labelsjson[set_].update(subjson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write finaljson as split.json file\n",
    "import json\n",
    "with open(f'{ROOT}/dataset-cartes/synth_maps_dataset/labels.json', 'w') as f:\n",
    "    json.dump(labelsjson, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create charset.pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open the labels.json file and create a new file named charset.pkl that contains the characters used in the text field of the labels.json file\n",
    "import pickle\n",
    "import json\n",
    "with open(f'{ROOT}/dataset-cartes/synth_maps_dataset/labels.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    text = \"\"\n",
    "    for key in data:\n",
    "        for k in data[key]:\n",
    "            text += data[key][k]\n",
    "    charset = set(text)\n",
    "    with open(f'{ROOT}/dataset-cartes/synth_maps_dataset/charset.pkl', 'wb') as f:\n",
    "        pickle.dump(charset, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get some statistics about DAN (help tot set config.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!teklia-dan dataset analyze \\\n",
    "    --labels /home/STual/DAN-cadastre/dataset-cartes/synth_maps_dataset/labels.json \\\n",
    "    --tokens /home/STual/DAN-cadastre/dataset-cartes/tokens.yml \\\n",
    "    --output-file /home/STual/DAN-cadastre/dataset-cartes/statistics.md"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
